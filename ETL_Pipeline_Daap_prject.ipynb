{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d647b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if fromJsonToMongo(myJsonFileloc=DATASETS/black_friday.json, myconnString=mongodb+srv://DAAP:ncidaap@atlascluster.tlybx6o.mongodb.net/?retryWrites=true&w=majority&appName=AtlasCluster, dataBsName=black_fri_db, collnName=black_fri, taskno=1) is complete\n",
      "INFO: Informed scheduler that task   fromJsonToMongo_black_fri_black_fri_db_DATASETS_black_f_8e9f89a7e2   has status   PENDING\n",
      "DEBUG: Checking if fromJsonToMongo(myJsonFileloc=DATASETS/superStore_analysis.json, myconnString=mongodb+srv://DAAP:ncidaap@atlascluster.tlybx6o.mongodb.net/?retryWrites=true&w=majority&appName=AtlasCluster, dataBsName=superstore_sls_db, collnName=super_sls, taskno=2) is complete\n",
      "INFO: Informed scheduler that task   fromJsonToMongo_super_sls_superstore_sls_d_DATASETS_superSt_c19d969d44   has status   PENDING\n",
      "DEBUG: Checking if readDatafrmAPI() is complete\n",
      "INFO: Informed scheduler that task   readDatafrmAPI__99914b932b   has status   PENDING\n",
      "DEBUG: Checking if retrieveDataAndCreateDataFramesuperstore(myconnString=mongodb+srv://DAAP:ncidaap@atlascluster.tlybx6o.mongodb.net/?retryWrites=true&w=majority&appName=AtlasCluster, dataBsName=superstore_sls_db, collnName=super_sls) is complete\n",
      "INFO: Informed scheduler that task   retrieveDataAndCreateDataFramesuperstore_super_sls_superstore_sls_d_mongodb_srv___DA_0d3e3949b3   has status   PENDING\n",
      "DEBUG: Checking if retrieveDataAndCreateDataFrameBlackFri(myconnString=mongodb+srv://DAAP:ncidaap@atlascluster.tlybx6o.mongodb.net/?retryWrites=true&w=majority&appName=AtlasCluster, dataBsName=black_fri_db, collnName=black_fri) is complete\n",
      "INFO: Informed scheduler that task   retrieveDataAndCreateDataFrameBlackFri_black_fri_black_fri_db_mongodb_srv___DA_138dda3970   has status   PENDING\n",
      "DEBUG: Checking if createDatabaseinPostgres(user=dap, password=dap, host=127.0.0.1, port=5432, database=postgres) is complete\n",
      "INFO: Informed scheduler that task   createDatabaseinPostgres_postgres_127_0_0_1_dap_3dd8ae9895   has status   PENDING\n",
      "DEBUG: Checking if saveSuperSalesDatatoPostgres(dataSetPath=outputDataframes/superStore_cleaned_data.csv, PostGres_table_name=superStoreSales, postgres_uri=postgresql://dap:dap@localhost:5432/dap, user=dap, password=dap, host=127.0.0.1, port=5432, database=daapproj) is complete\n",
      "INFO: Informed scheduler that task   saveSuperSalesDatatoPostgres_superStoreSales_outputDataframes_daapproj_508bd73833   has status   PENDING\n",
      "DEBUG: Checking if saveBlackFridayDatatoPostgres(dataSetPath=outputDataframes/black_friday_cleaned_data.csv, PostGres_table_name=superStoreSales, postgres_uri=postgresql://dap:dap@localhost:5432/dap, user=dap, password=dap, host=127.0.0.1, port=5432, database=daapproj) is complete\n",
      "INFO: Informed scheduler that task   saveBlackFridayDatatoPostgres_superStoreSales_outputDataframes_daapproj_ba6d7395d5   has status   PENDING\n",
      "DEBUG: Checking if saveAppleStockDatatoPostgres(dataSetPath=outputDataframes/apple_cleaned_data.csv, PostGres_table_name=superStoreSales, postgres_uri=postgresql://dap:dap@localhost:5432/dap, user=dap, password=dap, host=127.0.0.1, port=5432, database=daapproj) is complete\n",
      "INFO: Informed scheduler that task   saveAppleStockDatatoPostgres_superStoreSales_outputDataframes_daapproj_23d7f91d0f   has status   PENDING\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 9\n",
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) running   fromJsonToMongo(myJsonFileloc=DATASETS/black_friday.json, myconnString=mongodb+srv://DAAP:ncidaap@atlascluster.tlybx6o.mongodb.net/?retryWrites=true&w=majority&appName=AtlasCluster, dataBsName=black_fri_db, collnName=black_fri, taskno=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Now Running Task 1 of Reading the json & saving it to MongoDB\n",
      "Now Inserting data in black_fri collection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) done      fromJsonToMongo(myJsonFileloc=DATASETS/black_friday.json, myconnString=mongodb+srv://DAAP:ncidaap@atlascluster.tlybx6o.mongodb.net/?retryWrites=true&w=majority&appName=AtlasCluster, dataBsName=black_fri_db, collnName=black_fri, taskno=1)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   fromJsonToMongo_black_fri_black_fri_db_DATASETS_black_f_8e9f89a7e2   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 8\n",
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) running   fromJsonToMongo(myJsonFileloc=DATASETS/superStore_analysis.json, myconnString=mongodb+srv://DAAP:ncidaap@atlascluster.tlybx6o.mongodb.net/?retryWrites=true&w=majority&appName=AtlasCluster, dataBsName=superstore_sls_db, collnName=super_sls, taskno=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records in black_fri collection: 550068 \n",
      "\n",
      "\n",
      "Now Running Task 2 of Reading the json & saving it to MongoDB\n",
      "Deleting existing data from superstore_sls_db in collection...\n",
      "Now Inserting data in super_sls collection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) done      fromJsonToMongo(myJsonFileloc=DATASETS/superStore_analysis.json, myconnString=mongodb+srv://DAAP:ncidaap@atlascluster.tlybx6o.mongodb.net/?retryWrites=true&w=majority&appName=AtlasCluster, dataBsName=superstore_sls_db, collnName=super_sls, taskno=2)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   fromJsonToMongo_super_sls_superstore_sls_d_DATASETS_superSt_c19d969d44   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 7\n",
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) running   readDatafrmAPI()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records in super_sls collection: 9800 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) done      readDatafrmAPI()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   readDatafrmAPI__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 6\n",
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) running   retrieveDataAndCreateDataFramesuperstore(myconnString=mongodb+srv://DAAP:ncidaap@atlascluster.tlybx6o.mongodb.net/?retryWrites=true&w=majority&appName=AtlasCluster, dataBsName=superstore_sls_db, collnName=super_sls)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Running Task 3 i.e Reading Apple Stock Dataset from an API...\n",
      "Apple Dataset: \n",
      "         Date  Adj Close     Close      High       Low      Open       Volume  \\\n",
      "0  1996-05-28   0.203497  0.235491  0.243304  0.235491  0.238839  101852800.0   \n",
      "1  1996-05-29   0.191924  0.222098  0.234375  0.220982  0.234375  219520000.0   \n",
      "2  1996-05-30   0.196746  0.227679  0.229911  0.220982  0.222098  103465600.0   \n",
      "3  1996-05-31   0.201568  0.233259  0.237723  0.227679  0.228795  162646400.0   \n",
      "4  1996-06-03   0.190959  0.220982  0.232143  0.220982  0.231027  125462400.0   \n",
      "\n",
      "       MACD    Signal  Rolling mean Adj Close 20  ...  lag_41  lag_42  lag_43  \\\n",
      "0  0.002717  0.003506                   0.203907  ...    -1.0     1.0    -1.0   \n",
      "1  0.001364  0.003077                   0.204100  ...     1.0    -1.0     1.0   \n",
      "2  0.000673  0.002596                   0.204534  ...     1.0     1.0    -1.0   \n",
      "3  0.000509  0.002179                   0.205450  ...    -1.0     1.0     1.0   \n",
      "4 -0.000472  0.001649                   0.205788  ...    -1.0    -1.0     1.0   \n",
      "\n",
      "   lag_44  lag_45  lag_46  lag_47  lag_48  lag_49  lag_50  \n",
      "0    -1.0     1.0    -1.0    -1.0    -1.0     1.0     1.0  \n",
      "1    -1.0    -1.0     1.0    -1.0    -1.0    -1.0     1.0  \n",
      "2     1.0    -1.0    -1.0     1.0    -1.0    -1.0    -1.0  \n",
      "3    -1.0     1.0    -1.0    -1.0     1.0    -1.0    -1.0  \n",
      "4     1.0    -1.0     1.0    -1.0    -1.0     1.0    -1.0  \n",
      "\n",
      "[5 rows x 66 columns]\n",
      "\n",
      "\n",
      "Checking the format of the columns before datatypes Changes: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6142 entries, 0 to 6141\n",
      "Data columns (total 66 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Date                       6142 non-null   object \n",
      " 1   Adj Close                  6142 non-null   float64\n",
      " 2   Close                      6142 non-null   float64\n",
      " 3   High                       6142 non-null   float64\n",
      " 4   Low                        6142 non-null   float64\n",
      " 5   Open                       6142 non-null   float64\n",
      " 6   Volume                     6142 non-null   float64\n",
      " 7   MACD                       6142 non-null   float64\n",
      " 8   Signal                     6142 non-null   float64\n",
      " 9   Rolling mean Adj Close 20  6142 non-null   float64\n",
      " 10  Rolling std Adj Close 20   6142 non-null   float64\n",
      " 11  Low 14                     6142 non-null   float64\n",
      " 12  High 14                    6142 non-null   float64\n",
      " 13  Williams %R                6142 non-null   float64\n",
      " 14  RSI                        6142 non-null   float64\n",
      " 15  Returns                    6142 non-null   float64\n",
      " 16  lag_1                      6142 non-null   float64\n",
      " 17  lag_2                      6142 non-null   float64\n",
      " 18  lag_3                      6142 non-null   float64\n",
      " 19  lag_4                      6142 non-null   float64\n",
      " 20  lag_5                      6142 non-null   float64\n",
      " 21  lag_6                      6142 non-null   float64\n",
      " 22  lag_7                      6142 non-null   float64\n",
      " 23  lag_8                      6142 non-null   float64\n",
      " 24  lag_9                      6142 non-null   float64\n",
      " 25  lag_10                     6142 non-null   float64\n",
      " 26  lag_11                     6142 non-null   float64\n",
      " 27  lag_12                     6142 non-null   float64\n",
      " 28  lag_13                     6142 non-null   float64\n",
      " 29  lag_14                     6142 non-null   float64\n",
      " 30  lag_15                     6142 non-null   float64\n",
      " 31  lag_16                     6142 non-null   float64\n",
      " 32  lag_17                     6142 non-null   float64\n",
      " 33  lag_18                     6142 non-null   float64\n",
      " 34  lag_19                     6142 non-null   float64\n",
      " 35  lag_20                     6142 non-null   float64\n",
      " 36  lag_21                     6142 non-null   float64\n",
      " 37  lag_22                     6142 non-null   float64\n",
      " 38  lag_23                     6142 non-null   float64\n",
      " 39  lag_24                     6142 non-null   float64\n",
      " 40  lag_25                     6142 non-null   float64\n",
      " 41  lag_26                     6142 non-null   float64\n",
      " 42  lag_27                     6142 non-null   float64\n",
      " 43  lag_28                     6142 non-null   float64\n",
      " 44  lag_29                     6142 non-null   float64\n",
      " 45  lag_30                     6142 non-null   float64\n",
      " 46  lag_31                     6142 non-null   float64\n",
      " 47  lag_32                     6142 non-null   float64\n",
      " 48  lag_33                     6142 non-null   float64\n",
      " 49  lag_34                     6142 non-null   float64\n",
      " 50  lag_35                     6142 non-null   float64\n",
      " 51  lag_36                     6142 non-null   float64\n",
      " 52  lag_37                     6142 non-null   float64\n",
      " 53  lag_38                     6142 non-null   float64\n",
      " 54  lag_39                     6142 non-null   float64\n",
      " 55  lag_40                     6142 non-null   float64\n",
      " 56  lag_41                     6142 non-null   float64\n",
      " 57  lag_42                     6142 non-null   float64\n",
      " 58  lag_43                     6142 non-null   float64\n",
      " 59  lag_44                     6142 non-null   float64\n",
      " 60  lag_45                     6142 non-null   float64\n",
      " 61  lag_46                     6142 non-null   float64\n",
      " 62  lag_47                     6142 non-null   float64\n",
      " 63  lag_48                     6142 non-null   float64\n",
      " 64  lag_49                     6142 non-null   float64\n",
      " 65  lag_50                     6142 non-null   float64\n",
      "dtypes: float64(65), object(1)\n",
      "memory usage: 3.1+ MB\n",
      "\n",
      "\n",
      "Checking the format of the columns after datatypes Changes: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6142 entries, 0 to 6141\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Date       6142 non-null   datetime64[ns]\n",
      " 1   Open       6142 non-null   float64       \n",
      " 2   High       6142 non-null   float64       \n",
      " 3   Low        6142 non-null   float64       \n",
      " 4   Close      6142 non-null   float64       \n",
      " 5   Adj Close  6142 non-null   float64       \n",
      " 6   Volume     6142 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(6)\n",
      "memory usage: 336.0 KB\n",
      "\n",
      "\n",
      "        Date      Open      High       Low     Close  Adj Close       Volume\n",
      "0 1996-05-28  0.238839  0.243304  0.235491  0.235491   0.203497  101852800.0\n",
      "1 1996-05-29  0.234375  0.234375  0.220982  0.222098   0.191924  219520000.0\n",
      "2 1996-05-30  0.222098  0.229911  0.220982  0.227679   0.196746  103465600.0\n",
      "we have now completed Data cleaning \n",
      "And the cleaned has been data saved to 'outputDataframes/apple_cleaned_data.csv'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now Running Task 4 i.e Reading Superstore Sales Dataset from an MongoDB and performing the Data Cleaning & transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) done      retrieveDataAndCreateDataFramesuperstore(myconnString=mongodb+srv://DAAP:ncidaap@atlascluster.tlybx6o.mongodb.net/?retryWrites=true&w=majority&appName=AtlasCluster, dataBsName=superstore_sls_db, collnName=super_sls)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   retrieveDataAndCreateDataFramesuperstore_super_sls_superstore_sls_d_mongodb_srv___DA_0d3e3949b3   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 5\n",
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) running   retrieveDataAndCreateDataFrameBlackFri(myconnString=mongodb+srv://DAAP:ncidaap@atlascluster.tlybx6o.mongodb.net/?retryWrites=true&w=majority&appName=AtlasCluster, dataBsName=black_fri_db, collnName=black_fri)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have retirevied data from mongoDB database and is saved in a Dataframe storeSlsDF \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9800 entries, 0 to 9799\n",
      "Data columns (total 19 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   _id            9800 non-null   object\n",
      " 1   Row ID         9800 non-null   object\n",
      " 2   Order ID       9800 non-null   object\n",
      " 3   Order Date     9800 non-null   object\n",
      " 4   Ship Date      9800 non-null   object\n",
      " 5   Ship Mode      9800 non-null   object\n",
      " 6   Customer ID    9800 non-null   object\n",
      " 7   Customer Name  9800 non-null   object\n",
      " 8   Segment        9800 non-null   object\n",
      " 9   Country        9800 non-null   object\n",
      " 10  City           9800 non-null   object\n",
      " 11  State          9800 non-null   object\n",
      " 12  Postal Code    9800 non-null   object\n",
      " 13  Region         9800 non-null   object\n",
      " 14  Product ID     9800 non-null   object\n",
      " 15  Category       9800 non-null   object\n",
      " 16  Sub-Category   9800 non-null   object\n",
      " 17  Product Name   9800 non-null   object\n",
      " 18  Sales          9800 non-null   object\n",
      "dtypes: object(19)\n",
      "memory usage: 1.4+ MB\n",
      "Columns type information after convertion of Sales Column datatype to Numeric :\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9800 entries, 0 to 9799\n",
      "Data columns (total 19 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   _id            9800 non-null   object \n",
      " 1   Row ID         9800 non-null   object \n",
      " 2   Order ID       9800 non-null   object \n",
      " 3   Order Date     9800 non-null   object \n",
      " 4   Ship Date      9800 non-null   object \n",
      " 5   Ship Mode      9800 non-null   object \n",
      " 6   Customer ID    9800 non-null   object \n",
      " 7   Customer Name  9800 non-null   object \n",
      " 8   Segment        9800 non-null   object \n",
      " 9   Country        9800 non-null   object \n",
      " 10  City           9800 non-null   object \n",
      " 11  State          9800 non-null   object \n",
      " 12  Postal Code    9800 non-null   object \n",
      " 13  Region         9800 non-null   object \n",
      " 14  Product ID     9800 non-null   object \n",
      " 15  Category       9800 non-null   object \n",
      " 16  Sub-Category   9800 non-null   object \n",
      " 17  Product Name   9800 non-null   object \n",
      " 18  Sales          9800 non-null   float64\n",
      "dtypes: float64(1), object(18)\n",
      "memory usage: 1.4+ MB\n",
      "\n",
      "\n",
      "Checking the format of date columns after datatype changes: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9800 entries, 0 to 9799\n",
      "Data columns (total 19 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   _id            9800 non-null   object        \n",
      " 1   Row ID         9800 non-null   object        \n",
      " 2   Order ID       9800 non-null   object        \n",
      " 3   Order Date     9800 non-null   datetime64[ns]\n",
      " 4   Ship Date      9800 non-null   datetime64[ns]\n",
      " 5   Ship Mode      9800 non-null   object        \n",
      " 6   Customer ID    9800 non-null   object        \n",
      " 7   Customer Name  9800 non-null   object        \n",
      " 8   Segment        9800 non-null   object        \n",
      " 9   Country        9800 non-null   object        \n",
      " 10  City           9800 non-null   object        \n",
      " 11  State          9800 non-null   object        \n",
      " 12  Postal Code    9800 non-null   object        \n",
      " 13  Region         9800 non-null   object        \n",
      " 14  Product ID     9800 non-null   object        \n",
      " 15  Category       9800 non-null   object        \n",
      " 16  Sub-Category   9800 non-null   object        \n",
      " 17  Product Name   9800 non-null   object        \n",
      " 18  Sales          9800 non-null   float64       \n",
      "dtypes: datetime64[ns](2), float64(1), object(16)\n",
      "memory usage: 1.4+ MB\n",
      "\n",
      "\n",
      "Arranging the dataframe according to the Order Date column\n",
      "                           _id Row ID        Order ID Order Date  Ship Date  \\\n",
      "7980  66313ac0d7d0d08e9e260043   7981  CA-2015-103800 2015-01-03 2015-01-07   \n",
      "741   66313ac0d7d0d08e9e25e3fc    742  CA-2015-112326 2015-01-04 2015-01-08   \n",
      "740   66313ac0d7d0d08e9e25e3fb    741  CA-2015-112326 2015-01-04 2015-01-08   \n",
      "\n",
      "           Ship Mode Customer ID  Customer Name      Segment        Country  \\\n",
      "7980  Standard Class    DP-13000  Darren Powers     Consumer  United States   \n",
      "741   Standard Class    PO-19195  Phillina Ober  Home Office  United States   \n",
      "740   Standard Class    PO-19195  Phillina Ober  Home Office  United States   \n",
      "\n",
      "            City     State Postal Code   Region       Product ID  \\\n",
      "7980     Houston     Texas       77095  Central  OFF-PA-10000174   \n",
      "741   Naperville  Illinois       60540  Central  OFF-BI-10004094   \n",
      "740   Naperville  Illinois       60540  Central  OFF-ST-10002743   \n",
      "\n",
      "             Category Sub-Category  \\\n",
      "7980  Office Supplies        Paper   \n",
      "741   Office Supplies      Binders   \n",
      "740   Office Supplies      Storage   \n",
      "\n",
      "                                           Product Name    Sales  \n",
      "7980  Message Book, Wirebound, Four 5 1/2\" X 4\" Form...   16.448  \n",
      "741          GBC Standard Plastic Binding Systems Combs    3.540  \n",
      "740                       SAFCO Boltless Steel Shelving  272.736  \n",
      "\n",
      "\n",
      "Results after removing rows in which the order date is later than the ship date\n",
      "                           _id Row ID        Order ID Order Date  Ship Date  \\\n",
      "7980  66313ac0d7d0d08e9e260043   7981  CA-2015-103800 2015-01-03 2015-01-07   \n",
      "741   66313ac0d7d0d08e9e25e3fc    742  CA-2015-112326 2015-01-04 2015-01-08   \n",
      "\n",
      "           Ship Mode Customer ID  Customer Name      Segment        Country  \\\n",
      "7980  Standard Class    DP-13000  Darren Powers     Consumer  United States   \n",
      "741   Standard Class    PO-19195  Phillina Ober  Home Office  United States   \n",
      "\n",
      "            City     State Postal Code   Region       Product ID  \\\n",
      "7980     Houston     Texas       77095  Central  OFF-PA-10000174   \n",
      "741   Naperville  Illinois       60540  Central  OFF-BI-10004094   \n",
      "\n",
      "             Category Sub-Category  \\\n",
      "7980  Office Supplies        Paper   \n",
      "741   Office Supplies      Binders   \n",
      "\n",
      "                                           Product Name   Sales  \n",
      "7980  Message Book, Wirebound, Four 5 1/2\" X 4\" Form...  16.448  \n",
      "741          GBC Standard Plastic Binding Systems Combs   3.540  \n",
      "\n",
      "\n",
      "Checking for all the Null values\n",
      "_id              0\n",
      "Row ID           0\n",
      "Order ID         0\n",
      "Order Date       0\n",
      "Ship Date        0\n",
      "Ship Mode        0\n",
      "Customer ID      0\n",
      "Customer Name    0\n",
      "Segment          0\n",
      "Country          0\n",
      "City             0\n",
      "State            0\n",
      "Postal Code      0\n",
      "Region           0\n",
      "Product ID       0\n",
      "Category         0\n",
      "Sub-Category     0\n",
      "Product Name     0\n",
      "Sales            0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Results for dublicates in the datasets\n",
      "0\n",
      "\n",
      "\n",
      "we have now completed Data cleaning \n",
      "And the cleaned has been data saved to 'outputDataframes/superStore_cleaned_data.csv'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now Running Task 5 i.e Reading Blackfriday Dataset from an MongoDB and performing the Data Cleaning & transformation...\n",
      "We have retirevied data from mongoDB database and is saved in a Dataframe storeSlsDF \n",
      "\n",
      "                        _id  User_ID Product_ID Gender   Age Occupation  \\\n",
      "0  663137b6d7d0d08e9e1d7c62  1000001  P00069042      F  0-17         10   \n",
      "1  663137b6d7d0d08e9e1d7c63  1000001  P00248942      F  0-17         10   \n",
      "\n",
      "  City_Category Stay_In_Current_City_Years Marital_Status Product_Category_1  \\\n",
      "0             A                          2              0                  3   \n",
      "1             A                          2              0                  1   \n",
      "\n",
      "  Product_Category_2 Product_Category_3 Purchase  \n",
      "0                                           8370  \n",
      "1                  6                 14    15200  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550068 entries, 0 to 550067\n",
      "Data columns (total 13 columns):\n",
      " #   Column                      Non-Null Count   Dtype \n",
      "---  ------                      --------------   ----- \n",
      " 0   _id                         550068 non-null  object\n",
      " 1   User_ID                     550068 non-null  object\n",
      " 2   Product_ID                  550068 non-null  object\n",
      " 3   Gender                      550068 non-null  object\n",
      " 4   Age                         550068 non-null  object\n",
      " 5   Occupation                  550068 non-null  object\n",
      " 6   City_Category               550068 non-null  object\n",
      " 7   Stay_In_Current_City_Years  550068 non-null  object\n",
      " 8   Marital_Status              550068 non-null  object\n",
      " 9   Product_Category_1          550068 non-null  object\n",
      " 10  Product_Category_2          550068 non-null  object\n",
      " 11  Product_Category_3          550068 non-null  object\n",
      " 12  Purchase                    550068 non-null  object\n",
      "dtypes: object(13)\n",
      "memory usage: 54.6+ MB\n",
      "Columns type information after convertion of Sales Column datatype to Numeric :\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550068 entries, 0 to 550067\n",
      "Data columns (total 13 columns):\n",
      " #   Column                      Non-Null Count   Dtype \n",
      "---  ------                      --------------   ----- \n",
      " 0   _id                         550068 non-null  object\n",
      " 1   User_ID                     550068 non-null  object\n",
      " 2   Product_ID                  550068 non-null  object\n",
      " 3   Gender                      550068 non-null  object\n",
      " 4   Age                         550068 non-null  object\n",
      " 5   Occupation                  550068 non-null  object\n",
      " 6   City_Category               550068 non-null  object\n",
      " 7   Stay_In_Current_City_Years  550068 non-null  object\n",
      " 8   Marital_Status              550068 non-null  object\n",
      " 9   Product_Category_1          550068 non-null  object\n",
      " 10  Product_Category_2          550068 non-null  object\n",
      " 11  Product_Category_3          550068 non-null  object\n",
      " 12  Purchase                    550068 non-null  object\n",
      "dtypes: object(13)\n",
      "memory usage: 54.6+ MB\n",
      "\n",
      "\n",
      "Results after removing rows in which the order date is later than the ship date\n",
      "_id                           0\n",
      "User_ID                       0\n",
      "Product_ID                    0\n",
      "Gender                        0\n",
      "Age                           0\n",
      "Occupation                    0\n",
      "City_Category                 0\n",
      "Stay_In_Current_City_Years    0\n",
      "Marital_Status                0\n",
      "Product_Category_1            0\n",
      "Product_Category_2            0\n",
      "Product_Category_3            0\n",
      "Purchase                      0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Results for dublicates in the datasets\n",
      "0\n",
      "\n",
      "\n",
      "Columns After dropping  product category 2 and product category 3 \n",
      "   User_ID Product_ID Gender   Age Occupation City_Category  \\\n",
      "0  1000001  P00069042      F  0-17         10             A   \n",
      "1  1000001  P00248942      F  0-17         10             A   \n",
      "\n",
      "  Stay_In_Current_City_Years Marital_Status Product_Category_1 Purchase  \n",
      "0                          2              0                  3     8370  \n",
      "1                          2              0                  1    15200  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) done      retrieveDataAndCreateDataFrameBlackFri(myconnString=mongodb+srv://DAAP:ncidaap@atlascluster.tlybx6o.mongodb.net/?retryWrites=true&w=majority&appName=AtlasCluster, dataBsName=black_fri_db, collnName=black_fri)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   retrieveDataAndCreateDataFrameBlackFri_black_fri_black_fri_db_mongodb_srv___DA_138dda3970   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 4\n",
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) running   createDatabaseinPostgres(user=dap, password=dap, host=127.0.0.1, port=5432, database=postgres)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have now completed Data cleaning \n",
      "And the cleaned has been data saved to 'outputDataframes/black_friday_cleaned_data.csv'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now Running Task 6 i.e Creating a Database in Postgres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) done      createDatabaseinPostgres(user=dap, password=dap, host=127.0.0.1, port=5432, database=postgres)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   createDatabaseinPostgres_postgres_127_0_0_1_dap_3dd8ae9895   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 3\n",
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) running   saveSuperSalesDatatoPostgres(dataSetPath=outputDataframes/superStore_cleaned_data.csv, PostGres_table_name=superStoreSales, postgres_uri=postgresql://dap:dap@localhost:5432/dap, user=dap, password=dap, host=127.0.0.1, port=5432, database=daapproj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database is successfully Created\n",
      "\n",
      "\n",
      "Now Running Task 7 i.e Creating Tables for Superstore and storing data in the table..\n",
      "Now Creating a Table for SuperStore Sales Dataset in postgres\n",
      "Table has been created successfully\n",
      "Read the CSV Dataset successfully\n",
      "                        _id  Row ID        Order ID  Order Date   Ship Date  \\\n",
      "0  66313ac0d7d0d08e9e260043    7981  CA-2015-103800  2015-01-03  2015-01-07   \n",
      "1  66313ac0d7d0d08e9e25e3fc     742  CA-2015-112326  2015-01-04  2015-01-08   \n",
      "2  66313ac0d7d0d08e9e25e3fb     741  CA-2015-112326  2015-01-04  2015-01-08   \n",
      "3  66313ac0d7d0d08e9e25e3fa     740  CA-2015-112326  2015-01-04  2015-01-08   \n",
      "4  66313ac0d7d0d08e9e25e7f6    1760  CA-2015-141817  2015-01-05  2015-01-12   \n",
      "\n",
      "        Ship Mode Customer ID  Customer Name      Segment        Country  \\\n",
      "0  Standard Class    DP-13000  Darren Powers     Consumer  United States   \n",
      "1  Standard Class    PO-19195  Phillina Ober  Home Office  United States   \n",
      "2  Standard Class    PO-19195  Phillina Ober  Home Office  United States   \n",
      "3  Standard Class    PO-19195  Phillina Ober  Home Office  United States   \n",
      "4  Standard Class    MB-18085     Mick Brown     Consumer  United States   \n",
      "\n",
      "           City         State  Postal Code   Region       Product ID  \\\n",
      "0       Houston         Texas      77095.0  Central  OFF-PA-10000174   \n",
      "1    Naperville      Illinois      60540.0  Central  OFF-BI-10004094   \n",
      "2    Naperville      Illinois      60540.0  Central  OFF-ST-10002743   \n",
      "3    Naperville      Illinois      60540.0  Central  OFF-LA-10003223   \n",
      "4  Philadelphia  Pennsylvania      19143.0     East  OFF-AR-10003478   \n",
      "\n",
      "          Category Sub-Category  \\\n",
      "0  Office Supplies        Paper   \n",
      "1  Office Supplies      Binders   \n",
      "2  Office Supplies      Storage   \n",
      "3  Office Supplies       Labels   \n",
      "4  Office Supplies          Art   \n",
      "\n",
      "                                        Product Name    Sales  \n",
      "0  Message Book, Wirebound, Four 5 1/2\" X 4\" Form...   16.448  \n",
      "1         GBC Standard Plastic Binding Systems Combs    3.540  \n",
      "2                      SAFCO Boltless Steel Shelving  272.736  \n",
      "3                                          Avery 508   11.784  \n",
      "4  Avery Hi-Liter EverBold Pen Style Fluorescent ...   19.536  \n",
      "\n",
      "\n",
      "After Renaming the columns in the storeSlsDF:\n",
      "   row_id        order_id  order_date   ship_date       ship_mode customer_id  \\\n",
      "0    7981  CA-2015-103800  2015-01-03  2015-01-07  Standard Class    DP-13000   \n",
      "1     742  CA-2015-112326  2015-01-04  2015-01-08  Standard Class    PO-19195   \n",
      "2     741  CA-2015-112326  2015-01-04  2015-01-08  Standard Class    PO-19195   \n",
      "\n",
      "   customer_name      segment        country        city     state  \\\n",
      "0  Darren Powers     Consumer  United States     Houston     Texas   \n",
      "1  Phillina Ober  Home Office  United States  Naperville  Illinois   \n",
      "2  Phillina Ober  Home Office  United States  Naperville  Illinois   \n",
      "\n",
      "   postal_code   region       product_id         category sub_category  \\\n",
      "0      77095.0  Central  OFF-PA-10000174  Office Supplies        Paper   \n",
      "1      60540.0  Central  OFF-BI-10004094  Office Supplies      Binders   \n",
      "2      60540.0  Central  OFF-ST-10002743  Office Supplies      Storage   \n",
      "\n",
      "                                        product_name    sales  \n",
      "0  Message Book, Wirebound, Four 5 1/2\" X 4\" Form...   16.448  \n",
      "1         GBC Standard Plastic Binding Systems Combs    3.540  \n",
      "2                      SAFCO Boltless Steel Shelving  272.736  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) done      saveSuperSalesDatatoPostgres(dataSetPath=outputDataframes/superStore_cleaned_data.csv, PostGres_table_name=superStoreSales, postgres_uri=postgresql://dap:dap@localhost:5432/dap, user=dap, password=dap, host=127.0.0.1, port=5432, database=daapproj)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   saveSuperSalesDatatoPostgres_superStoreSales_outputDataframes_daapproj_508bd73833   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 2\n",
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) running   saveBlackFridayDatatoPostgres(dataSetPath=outputDataframes/black_friday_cleaned_data.csv, PostGres_table_name=superStoreSales, postgres_uri=postgresql://dap:dap@localhost:5432/dap, user=dap, password=dap, host=127.0.0.1, port=5432, database=daapproj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the data has been saved to the superstore_sls_tbl tables successfully\n",
      "\n",
      "\n",
      "Now Running Task 8 i.e Creating Tables for BlackFriday and storing data in the postgres table..\n",
      "Table has been created successfully\n",
      "Read the CSV Dataset successfully\n",
      "Black Friday Cleaned CSV retreived :\n",
      "   User_ID Product_ID Gender   Age  Occupation City_Category  \\\n",
      "0  1000001  P00069042      F  0-17          10             A   \n",
      "1  1000001  P00248942      F  0-17          10             A   \n",
      "2  1000001  P00087842      F  0-17          10             A   \n",
      "\n",
      "  Stay_In_Current_City_Years  Marital_Status  Product_Category_1  Purchase  \n",
      "0                          2               0                   3      8370  \n",
      "1                          2               0                   1     15200  \n",
      "2                          2               0                  12      1422  \n",
      "Column Names After Renaming the column from the dataset\n",
      "Index(['user_id', 'product_id', 'gender', 'age', 'occupation', 'city_category',\n",
      "       'stay_in_current_city_years', 'marital_status', 'product_category_1',\n",
      "       'purchase'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) done      saveBlackFridayDatatoPostgres(dataSetPath=outputDataframes/black_friday_cleaned_data.csv, PostGres_table_name=superStoreSales, postgres_uri=postgresql://dap:dap@localhost:5432/dap, user=dap, password=dap, host=127.0.0.1, port=5432, database=daapproj)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   saveBlackFridayDatatoPostgres_superStoreSales_outputDataframes_daapproj_ba6d7395d5   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) running   saveAppleStockDatatoPostgres(dataSetPath=outputDataframes/apple_cleaned_data.csv, PostGres_table_name=superStoreSales, postgres_uri=postgresql://dap:dap@localhost:5432/dap, user=dap, password=dap, host=127.0.0.1, port=5432, database=daapproj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the data has been saved to the blackfri_tbl tables successfully\n",
      "\n",
      "\n",
      "Now Running Task 9 i.e Creating Tables for BlackFriday and Storing data in the postgres table..\n",
      "Table has been created successfully\n",
      "\n",
      "\n",
      "apple DF Cleaned CSV retreived and read the CSV Dataset successfully\n",
      "         Date      Open      High       Low     Close  Adj Close       Volume\n",
      "0  1996-05-28  0.238839  0.243304  0.235491  0.235491   0.203497  101852800.0\n",
      "1  1996-05-29  0.234375  0.234375  0.220982  0.222098   0.191924  219520000.0\n",
      "2  1996-05-30  0.222098  0.229911  0.220982  0.227679   0.196746  103465600.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 75041] Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) done      saveAppleStockDatatoPostgres(dataSetPath=outputDataframes/apple_cleaned_data.csv, PostGres_table_name=superStoreSales, postgres_uri=postgresql://dap:dap@localhost:5432/dap, user=dap, password=dap, host=127.0.0.1, port=5432, database=daapproj)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   saveAppleStockDatatoPostgres_superStoreSales_outputDataframes_daapproj_23d7f91d0f   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=9769424971, workers=1, host=Anikets-MacBook-Air.local, username=aniketshetty, pid=75041) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 9 tasks of which:\n",
      "* 9 ran successfully:\n",
      "    - 1 createDatabaseinPostgres(user=dap, password=dap, host=127.0.0.1, port=5432, database=postgres)\n",
      "    - 2 fromJsonToMongo(...)\n",
      "    - 1 readDatafrmAPI()\n",
      "    - 1 retrieveDataAndCreateDataFrameBlackFri(...)\n",
      "    - 1 retrieveDataAndCreateDataFramesuperstore(...)\n",
      "    ...\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the data has been saved to the aapl_tbl tables successfully\n"
     ]
    }
   ],
   "source": [
    "# Importing all the necessary packages\n",
    "\n",
    "import luigi # luigi class for our \n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from pymongo.errors import ConnectionFailure\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# Below class is for reading the JSON file then saving it to our MongoDB Atlas Database\n",
    "# Operation 1 & 2\n",
    "class fromJsonToMongo(luigi.Task):\n",
    "\n",
    "# Our necessary parmeters which are required for the function\n",
    "    myJsonFileloc = luigi.Parameter()\n",
    "    myconnString = luigi.Parameter()\n",
    "    dataBsName = luigi.Parameter()\n",
    "    collnName = luigi.Parameter()\n",
    "    taskno = luigi.Parameter()\n",
    "     \n",
    "# Defining a new function that we want to perform for insertion of json data into mongodb\n",
    "    def run(self):\n",
    "        # Connect to MongoDB Atlas\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(f\"Now Running Task {self.taskno} of Reading the json & saving it to MongoDB\")\n",
    "           \n",
    "        client = MongoClient(self.myconnString)\n",
    "        SAA_db = client.get_database(self.dataBsName)\n",
    "        SAA_collection = SAA_db.get_collection(self.collnName)\n",
    "        \n",
    "        # before inserting into the Databse we will first Check if collection is empty or no\n",
    "        # if not empty thenn we will delete it.\n",
    "        if SAA_collection.count_documents({}) > 0:\n",
    "            print(f'Deleting existing data from {self.dataBsName} in collection...')\n",
    "            SAA_collection.delete_many({})\n",
    "\n",
    "        # open json file to read the contents that we want to save to mongo DB\n",
    "        with open(self.myJsonFileloc, 'r') as jfile:\n",
    "            # Load the JSON data\n",
    "            myJSONData = json.load(jfile)\n",
    "            # Insert the data into MongoDB collection\n",
    "            print(f'Now Inserting data in {self.collnName} collection...')\n",
    "            SAA_collection.insert_many(myJSONData)\n",
    "            \n",
    "        print(f\"Records in {self.collnName} collection: {SAA_collection.count_documents({})} \")\n",
    "\n",
    "        \n",
    "\n",
    "# Below Class is for reading From the API's then performing Data cleaning and saving it in the csv File\n",
    "# Operation 3\n",
    "class readDatafrmAPI(luigi.Task):\n",
    "    def run(self):\n",
    "        try:\n",
    "    \n",
    "            # URL of the raw CSV file in the GitHub repository\n",
    "            csv_url = \"https://raw.githubusercontent.com/MariaMoor/Investment-and-Trading-Capstone-Project/master/Capstone%20data\"\n",
    "\n",
    "            # we will send a GET request to the URL\n",
    "            response = requests.get(csv_url)\n",
    "            \n",
    "            # let us Check if the request was successful (status code 200)\n",
    "            if response.status_code == 200:\n",
    "                # Decode the content if needed (CSV files are typically encoded as UTF-8).\n",
    "                decoded_content = response.content.decode('utf-8')\n",
    "                \n",
    "                \n",
    "                # Parse the CSV data using Pandas\n",
    "                appleDataframe = pd.read_csv(io.StringIO(decoded_content), delimiter=',')\n",
    "                print(\"Now Running Task 3 i.e Reading Apple Stock Dataset from an API...\")\n",
    "                print(\"Apple Dataset: \")\n",
    "                print(appleDataframe.head())  # Print the first few rows of the DataFrame\n",
    "                print('\\n')\n",
    "                \n",
    "                # Now we will work with the appleDataframe\n",
    "                print('Checking the format of the columns before datatypes Changes: ')\n",
    "                appleDataframe.info()\n",
    "                print('\\n')\n",
    "                # Keeping only required columns\n",
    "                appleDataframe = appleDataframe[['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]\n",
    "               \n",
    "                appleDataframe['Open'] = appleDataframe['Open'].astype(float)\n",
    "                appleDataframe['High'] = appleDataframe['High'].astype(float)\n",
    "                appleDataframe['Low'] = appleDataframe['Low'].astype(float)\n",
    "                appleDataframe['Close'] = appleDataframe['Close'].astype(float)\n",
    "                appleDataframe['Adj Close'] = appleDataframe['Adj Close'].astype(float)\n",
    "                appleDataframe['Volume'] = appleDataframe['Volume'].astype(float)\n",
    "                appleDataframe['Date'] = pd.to_datetime(appleDataframe['Date'], format='%Y-%m-%d')  \n",
    "                \n",
    "                 # Check the format of 'Date' column\n",
    "                print('Checking the format of the columns after datatypes Changes: ')\n",
    "                \n",
    "                appleDataframe.info()\n",
    "                print('\\n')\n",
    "                \n",
    "                print(appleDataframe.head(3))\n",
    "                \n",
    "                \n",
    "                # now we will save the cleaned DataFrame to a CSV file\n",
    "                # Create the folder if it does not exist\n",
    "                folder_path = 'outputDataframes'\n",
    "                if not os.path.exists(folder_path):\n",
    "                    os.makedirs(folder_path)\n",
    "\n",
    "                # Name of the CSV file path within the folder\n",
    "                csv_file_path = os.path.join(folder_path, 'apple_cleaned_data.csv')\n",
    "\n",
    "                # Save the DataFrame to CSV file\n",
    "                appleDataframe.to_csv(csv_file_path, index=False)\n",
    "\n",
    "                print(\"we have now completed Data cleaning \")\n",
    "                print(f\"And the cleaned has been data saved to '{folder_path}/apple_cleaned_data.csv'\")  \n",
    "                print('\\n')\n",
    "                \n",
    "            else:\n",
    "                print(\"There was some error while reading CSV from GIT API \")  \n",
    "                \n",
    "        except Exception as excep:\n",
    "            # Handle other types of exceptions\n",
    "            print(f\"Oops! Some Error Occured: {excep}\")\n",
    " \n",
    "\n",
    "\n",
    "# The below class is used to retireve Data from superstore Mongo Database and save to a Dataframe and procceding for datacleaning\n",
    "# Operation 4\n",
    "\n",
    "class retrieveDataAndCreateDataFramesuperstore(luigi.Task):\n",
    "\n",
    "    myconnString = luigi.Parameter()\n",
    "    dataBsName = luigi.Parameter()\n",
    "    collnName = luigi.Parameter()\n",
    "        \n",
    "    def run(self):\n",
    "        try:\n",
    "            # Connect to MongoDB Atlas\n",
    "            client = MongoClient(self.myconnString)\n",
    "            dabse = client[self.dataBsName] # example superstore_sls_db or  is our database name\n",
    "            supSls_recrds = dabse[self.collnName] #this is our collection in which we will perform the Data analysis & EDA\n",
    "            print(\"\\n\")\n",
    "            print(\"Now Running Task 4 i.e Reading Superstore Sales Dataset from an MongoDB and performing the Data Cleaning & transformation...\")\n",
    "            \n",
    "            storeSlsDF = pd.DataFrame(list(supSls_recrds.find({})))  # Doing a find operation and storing the data from database in a dataframe\n",
    "            print('We have retirevied data from mongoDB database and is saved in a Dataframe storeSlsDF \\n')\n",
    "            storeSlsDF.head(2)\n",
    "            \n",
    "            # getting some information on column Type\n",
    "            storeSlsDF.info()\n",
    "            \n",
    "            # Lets convert the Sales column datatype to Numeric for our sales analysis as its datatype is in Object\n",
    "            storeSlsDF['Sales'] = pd.to_numeric(storeSlsDF['Sales'], errors='coerce')  #here 'coerce' will result in NaN conversion of non-convertible values.\n",
    "            print('Columns type information after convertion of Sales Column datatype to Numeric :\\n')\n",
    "            storeSlsDF.info()\n",
    "            print('\\n')\n",
    "            \n",
    "            # Since the \"Order Date\" and \"Ship Date\" columns' datatypes are both objects, let's convert them to a datetime format.\n",
    "            storeSlsDF['Order Date'] = pd.to_datetime(storeSlsDF['Order Date'], format='%d/%m/%Y')  \n",
    "            storeSlsDF['Ship Date']= pd.to_datetime(storeSlsDF['Ship Date'], format='%d/%m/%Y')\n",
    "            \n",
    "            \n",
    "            # Check the format of 'Date' column\n",
    "            print('Checking the format of date columns after datatype changes: ')\n",
    "            storeSlsDF.info()\n",
    "            print('\\n')\n",
    "            \n",
    "            #Let's now arrange the data according to the Order Date column.\n",
    "            storeSlsDF = storeSlsDF.sort_values(by=\"Order Date\")\n",
    "            print('Arranging the dataframe according to the Order Date column')\n",
    "            print(storeSlsDF.head(3))\n",
    "            print('\\n')\n",
    "            \n",
    "            #remove rows in which the order date is later than the ship date\n",
    "            storeSlsDF = storeSlsDF[storeSlsDF['Ship Date'] >= storeSlsDF['Order Date']]\n",
    "            print(\"Results after removing rows in which the order date is later than the ship date\")\n",
    "            print(storeSlsDF.head(2))\n",
    "            print('\\n')\n",
    "            \n",
    "            #lets have a look at columns that shows us how many values are missing.\n",
    "            print(\"Checking for all the Null values\")\n",
    "            print(storeSlsDF.isna().sum())\n",
    "            print('\\n')\n",
    "            \n",
    "            #lets check if we have any duplicates\n",
    "            print(\"Results for dublicates in the datasets\")\n",
    "            print(storeSlsDF.duplicated().sum())\n",
    "            print('\\n')\n",
    "            \n",
    "            # now we will save the cleaned DataFrame to a CSV file\n",
    "            # Create the folder if it does not exist\n",
    "            folder_path = 'outputDataframes'\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "            \n",
    "            # Define the CSV file path within the folder\n",
    "            csv_file_path = os.path.join(folder_path, 'superStore_cleaned_data.csv')\n",
    "\n",
    "            # Save the DataFrame to CSV file\n",
    "            storeSlsDF.to_csv(csv_file_path, index=False)\n",
    "            \n",
    "            print(\"we have now completed Data cleaning \")\n",
    "            print(f\"And the cleaned has been data saved to '{folder_path}/superStore_cleaned_data.csv'\")  \n",
    "            print('\\n')\n",
    "                \n",
    "                \n",
    "            \n",
    "        except ConnectionFailure as ConFailure:  # here we will Handle connection failure exceptions\n",
    "            print(f\"MongoDB Atlas connection failed: {ConFailure}\")\n",
    "        except Exception as excep:\n",
    "            # Handle other types of exceptions\n",
    "            print(f\"Oops! Some Error Occured: {excep}\")\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# The below class is used to retireve Data from BlackFriday Mongo Database and save to a Dataframe and procceding for datacleaning\n",
    "# Operation 5\n",
    "class retrieveDataAndCreateDataFrameBlackFri(luigi.Task):\n",
    " \n",
    "    myconnString = luigi.Parameter()\n",
    "    dataBsName = luigi.Parameter()\n",
    "    collnName = luigi.Parameter()\n",
    "    def run(self):\n",
    "        try:\n",
    "            \n",
    "#             with open('DATASETS/black_friday.json', 'r') as jfile:\n",
    "#                 # Load the JSON data\n",
    "#                 myJSONData = json.load(jfile)\n",
    "            \n",
    "#             myJSONData = pd.DataFabsrame(myJSONData)\n",
    "            print(\"\\n\")\n",
    "            print(\"Now Running Task 5 i.e Reading Blackfriday Dataset from an MongoDB and performing the Data Cleaning & transformation...\")\n",
    "\n",
    "            # Connect to MongoDB Atlas\n",
    "            client = MongoClient(self.myconnString)\n",
    "            dabse = client[self.dataBsName] # example superstore_sls_db or  is our database name\n",
    "            bf_recrds = dabse[self.collnName] #this is our collection in which we will perform the Data analysis & EDA\n",
    "            bf_DF = pd.DataFrame(list(bf_recrds.find({})))  # Doing a find operation and storing the data from database in a dataframe\n",
    "#             bf_DF = myJSONData\n",
    "            print('We have retirevied data from mongoDB database and is saved in a Dataframe storeSlsDF \\n')\n",
    "            print(bf_DF.head(2))\n",
    "            # getting some information on column Type\n",
    "            bf_DF.info()\n",
    "            \n",
    "            bf_DF['Product_ID'] = bf_DF['Product_ID'].astype(str)\n",
    "            bf_DF['Gender'] = bf_DF['Gender'].astype(str)\n",
    "            bf_DF['Age'] = bf_DF['Age'].astype(str)\n",
    "            bf_DF['City_Category'] = bf_DF['City_Category'].astype(str) \n",
    "            bf_DF['Stay_In_Current_City_Years'] = bf_DF['Stay_In_Current_City_Years'].astype(str)\n",
    "            print('Columns type information after convertion of Sales Column datatype to Numeric :\\n')\n",
    "            bf_DF.info()\n",
    "            print('\\n')\n",
    "            \n",
    "            \n",
    "            #lets have a look at columns that shows us how many values are missing.\n",
    "            print(\"Results after removing rows in which the order date is later than the ship date\")\n",
    "            print(bf_DF.isna().sum())\n",
    "            print('\\n')\n",
    "            \n",
    "            #lets check if we have any duplicates\n",
    "            print(\"Results for dublicates in the datasets\")\n",
    "            print(bf_DF.duplicated().sum())\n",
    "            print('\\n')\n",
    "            \n",
    "           \n",
    "            #Dropping two columns\n",
    "            #If a column has more than 30% missing values, we drop it. In this case, as can be seen in the plot, product category 2 and product category 3 have high levels of missing values.\n",
    "            bf_DF = bf_DF.drop(['Product_Category_2', 'Product_Category_3','_id'], axis=1)\n",
    "            print(\"Columns After dropping  product category 2 and product category 3 \")\n",
    "            print(bf_DF.head(2))\n",
    "            \n",
    "#              bf_DF = bf_DF.reset_index(inplace=True)\n",
    "                \n",
    "            # Save cleaned DataFrame to CSV file\n",
    "            # Create the folder if it does not exist\n",
    "            folder_path = 'outputDataframes'\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "            # Define the CSV file path within the folder\n",
    "            csv_file_path = os.path.join(folder_path, 'black_friday_cleaned_data.csv')\n",
    " \n",
    "            # Save the DataFrame to CSV file\n",
    "            bf_DF.to_csv(csv_file_path, index=False)\n",
    "            print(\"we have now completed Data cleaning \")\n",
    "            print(f\"And the cleaned has been data saved to '{folder_path}/black_friday_cleaned_data.csv'\")  \n",
    "            print('\\n')\n",
    "        except ConnectionFailure as CF:   # here we will Handle connection failure exceptions\n",
    "            print(f\"MongoDB Atlas connection failed: {CF}\")\n",
    "        except Exception as excep:\n",
    "            # Handle other types of exceptions\n",
    "            print(f\"Oops! Some Error Occured: {excep}\")\n",
    "\n",
    "# below class is for creating a New Database in the Postgres SQL Database if not created already.\n",
    "# If database already exist then delete the database and create the database\n",
    "# Operation 6\n",
    "class createDatabaseinPostgres(luigi.Task):\n",
    "    user = luigi.Parameter()\n",
    "    password = luigi.Parameter()\n",
    "    host = luigi.Parameter()\n",
    "    port = luigi.Parameter()\n",
    "    database = luigi.Parameter()\n",
    "    \n",
    "    def run(self):\n",
    "      \n",
    "        # The following code will establish a connection to Postgres and then create a database:\n",
    "        try:\n",
    "            # Load CSV data into a DataFrame\n",
    "            dbConnstr = psycopg2.connect(\n",
    "                user = self.user,\n",
    "                password = self.password,\n",
    "                host = self.host,\n",
    "                port =  self.port,\n",
    "                database =self.database\n",
    "            )\n",
    "\n",
    "            print(\"\\n\")\n",
    "            print(\"Now Running Task 6 i.e Creating a Database in Postgres...\")\n",
    "\n",
    "            \n",
    "            # This will do a autocommit    \n",
    "            dbConnstr.set_isolation_level(0)\n",
    "                    # The following code will create a cursor:\n",
    "            dbCursor = dbConnstr.cursor() \n",
    "            # Cursor will drop the database if it exists:\n",
    "            dbCursor.execute('DROP DATABASE IF EXISTS daapproj;')\n",
    "            # Cursor will create a database we want:\n",
    "            dbCursor.execute('CREATE DATABASE daapproj;')    \n",
    "            print(\"Database is successfully Created\")\n",
    "                # Closing the cursor:\n",
    "            dbCursor.close()\n",
    "            \n",
    "        except (Exception , psycopg2.Error) as dbError :\n",
    "            print (\"An error has occured while connecting to the PostgreSQL database: \", dbError)\n",
    "        finally:\n",
    "            # Closing the postgres connection:\n",
    "            if dbConnstr in locals(): \n",
    "                dbConnstr.close()\n",
    "                        \n",
    "            \n",
    "            \n",
    "## Now we will read the CSV File and then Save data to postgresSQL for SuperStore Sales analysis Dataset \n",
    "# Operation 7\n",
    "class saveSuperSalesDatatoPostgres(luigi.Task):\n",
    "\n",
    "    dataSetPath = luigi.Parameter()\n",
    "    PostGres_table_name = luigi.Parameter()\n",
    "    postgres_uri = luigi.Parameter()\n",
    "    user = luigi.Parameter()\n",
    "    password = luigi.Parameter()\n",
    "    host = luigi.Parameter()\n",
    "    port = luigi.Parameter()\n",
    "    database = luigi.Parameter()\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        # The following code will establish a connection to Postgres and then create a database:\n",
    "        try:\n",
    "\n",
    "            print(\"\\n\")\n",
    "            print(\"Now Running Task 7 i.e Creating Tables for Superstore and storing data in the table..\")\n",
    "\n",
    "            dbConnstr = psycopg2.connect(\n",
    "                user = self.user,\n",
    "                password = self.password,\n",
    "                host = self.host,\n",
    "                port =  self.port,\n",
    "                database =self.database\n",
    "            )\n",
    "            \n",
    "#             Query for our tabel \n",
    "            table_create_string = \"\"\" \n",
    "            DROP TABLE IF EXISTS superstore_sls_tbl;\n",
    "            CREATE TABLE IF NOT EXISTS superstore_sls_tbl (\n",
    "            row_id INTEGER,\n",
    "            order_id VARCHAR(100),\n",
    "            order_date DATE,\n",
    "            ship_date DATE,\n",
    "            ship_mode VARCHAR(100),\n",
    "            customer_id VARCHAR(100),\n",
    "            customer_name VARCHAR(100),\n",
    "            segment VARCHAR(100),\n",
    "            country VARCHAR(100),\n",
    "            city VARCHAR(100),\n",
    "            state VARCHAR(100),\n",
    "            postal_code VARCHAR(100),\n",
    "            region VARCHAR(100),\n",
    "            product_id VARCHAR(100),\n",
    "            category VARCHAR(100),\n",
    "            sub_category VARCHAR(100),\n",
    "            product_name VARCHAR(500),\n",
    "            sales numeric(20,8)\n",
    "            ); \"\"\"\n",
    "            \n",
    "            \n",
    "            # This will establish a connection to the postgres database:\n",
    "    \n",
    "            print(\"Now Creating a Table for SuperStore Sales Dataset in postgres\")\n",
    "            # This will do a autocommit    \n",
    "            dbConnstr.set_isolation_level(0)\n",
    "            \n",
    "            # The following code will create a cursor:\n",
    "            dbCursor = dbConnstr.cursor() \n",
    "            \n",
    "            # The following code will create the table in the respective database:\n",
    "            dbCursor.execute(table_create_string)  \n",
    "            \n",
    "            # Print the success message of table creation:\n",
    "            print(\"Table has been created successfully\")\n",
    "\n",
    "            # Now  Lets read the cleaned csv data and convert it to dataframe\n",
    "            # Path to the CSV file \n",
    "#             superStore_cleaned_data = \"outputDataframes/superStore_cleaned_data.csv\"\n",
    "            superStore_cleaned_data =  self.dataSetPath\n",
    "            # Read the CSV file into a \n",
    "            storeSlsDF = pd.read_csv(superStore_cleaned_data) \n",
    "            # Display the DataFrame\n",
    "            print(\"Read the CSV Dataset successfully\")\n",
    "\n",
    "            print(storeSlsDF.head(5))\n",
    "            \n",
    "            storeSlsDF.drop('_id', axis=1, inplace=True)\n",
    "            storeSlsDF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            storeSlsDF.rename(columns={'Row ID': 'row_id', 'Order ID': 'order_id','Order Date': 'order_date','Ship Date': 'ship_date',\n",
    "                      'Ship Mode': 'ship_mode','Customer ID': 'customer_id','Customer Name': 'customer_name','Postal Code': 'postal_code',\n",
    "                      'Product ID': 'product_id','Sub-Category': 'sub_category',\n",
    "                      'Product Name': 'product_name','Segment': 'segment','Country': 'country','City': 'city',\n",
    "                      'State': 'state','Region': 'region','Category': 'category','Sales': 'sales'}, inplace=True)\n",
    "            print(\"\\n\")\n",
    "            print(\"After Renaming the columns in the storeSlsDF:\")\n",
    "            print(storeSlsDF.head(3))\n",
    "            \n",
    "            # Type casting of the Data frame :\n",
    "            storeSlsDF['row_id'] = storeSlsDF['row_id'].astype(int)\n",
    "            storeSlsDF['order_id'] = storeSlsDF['order_id'].astype(str)\n",
    "            storeSlsDF['ship_mode'] = storeSlsDF['ship_mode'].astype(str)\n",
    "            storeSlsDF['customer_id'] = storeSlsDF['customer_id'].astype(str)\n",
    "            storeSlsDF['customer_name'] = storeSlsDF['customer_name'].astype(str)\n",
    "            storeSlsDF['segment'] = storeSlsDF['segment'].astype(str)\n",
    "            storeSlsDF['country'] = storeSlsDF['country'].astype(str)\n",
    "            storeSlsDF['city'] = storeSlsDF['city'].astype(str)\n",
    "            storeSlsDF['state'] = storeSlsDF['state'].astype(str)\n",
    "            storeSlsDF['postal_code'] = storeSlsDF['postal_code'].astype(str)\n",
    "            storeSlsDF['region'] = storeSlsDF['region'].astype(str)\n",
    "            storeSlsDF['product_id'] = storeSlsDF['product_id'].astype(str)\n",
    "            storeSlsDF['category'] = storeSlsDF['category'].astype(str)\n",
    "            storeSlsDF['sub_category'] = storeSlsDF['sub_category'].astype(str)\n",
    "            storeSlsDF['product_name'] = storeSlsDF['product_name'].astype(str)\n",
    "\n",
    "            # Mention the name of the target table:\n",
    "            table_name = 'superstore_sls_tbl'\n",
    "            # Prepare the insert query:\n",
    "            insert_query = sql.SQL(\"INSERT INTO {} ({}) VALUES ({})\").format(\n",
    "                sql.Identifier(table_name),\n",
    "                sql.SQL(', ').join(map(sql.Identifier, storeSlsDF.columns)),\n",
    "                sql.SQL(', ').join(sql.Placeholder() * len(storeSlsDF.columns))\n",
    "            )\n",
    "            # Converting the data from dataframe into list:\n",
    "            values = [tuple(x) for x in storeSlsDF.to_numpy()]\n",
    "            # Insert the values into the table:\n",
    "            dbCursor.executemany(insert_query, values)\n",
    "            # This will do a autocommit    \n",
    "            dbConnstr.set_isolation_level(0) \n",
    "            print(\"All the data has been saved to the superstore_sls_tbl tables successfully\")\n",
    "            \n",
    "        except (Exception , psycopg2.Error) as dbError :\n",
    "            print (\"An error has occured while connecting to the PostgreSQL database: \", dbError)\n",
    "        finally:\n",
    "            # Closing the postgres connection:\n",
    "            if dbConnstr in locals(): \n",
    "                dbConnstr.close()\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "# Now we will save the cleaned dataset BlackFriday to Postgres\n",
    "# Operation 8\n",
    "class saveBlackFridayDatatoPostgres(luigi.Task):  \n",
    "                \n",
    "           \n",
    "        dataSetPath = luigi.Parameter()\n",
    "        PostGres_table_name = luigi.Parameter()\n",
    "        postgres_uri = luigi.Parameter()\n",
    "        user = luigi.Parameter()\n",
    "        password = luigi.Parameter()\n",
    "        host = luigi.Parameter()\n",
    "        port = luigi.Parameter()\n",
    "        database = luigi.Parameter()\n",
    "\n",
    "        def run(self):\n",
    "\n",
    "            \n",
    "            # The following code will establish a connection to Postgres and then create a database:\n",
    "            try:\n",
    "                print(\"\\n\")\n",
    "                print(\"Now Running Task 8 i.e Creating Tables for BlackFriday and storing data in the postgres table..\")\n",
    "                table_create_string = \"\"\" \n",
    "                DROP TABLE IF EXISTS blackfri_tbl;\n",
    "                CREATE TABLE IF NOT EXISTS blackfri_tbl (\n",
    "                user_id INTEGER, \n",
    "                product_id VARCHAR(100),\n",
    "                gender VARCHAR(100),\n",
    "                age VARCHAR(100),\n",
    "                occupation INTEGER,\n",
    "                city_category VARCHAR(100),\n",
    "                stay_in_current_city_years VARCHAR(100),\n",
    "                marital_status INTEGER,\n",
    "                product_category_1 INTEGER,\n",
    "                purchase numeric(20,8)\n",
    "                ); \"\"\"\n",
    "\n",
    "\n",
    "                dbConnstr = psycopg2.connect(\n",
    "                    user = self.user,\n",
    "                    password = self.password,\n",
    "                    host = self.host,\n",
    "                    port = self.port,\n",
    "                    database = self.database\n",
    "                )\n",
    "\n",
    "                # This will do a autocommit    \n",
    "                dbConnstr.set_isolation_level(0)\n",
    "                # The following code will create a cursor:\n",
    "                dbCursor = dbConnstr.cursor() \n",
    "                # The following code will create the table in the respective database:\n",
    "                dbCursor.execute(table_create_string)  \n",
    "                # Print the success message of table creation:\n",
    "                print(\"Table has been created successfully\")\n",
    "    #             # Closing the cursor:\n",
    "    #             dbCursor.close()\n",
    "\n",
    "\n",
    "                # Now  Lets read the cleaned csv data and convert it to dataframe\n",
    "                # Path to the CSV file \n",
    "                black_friday_cleaned_data = self.dataSetPath\n",
    "                # Read the CSV file into a \n",
    "                blkFriDF = pd.read_csv(black_friday_cleaned_data) \n",
    "                # Display the DataFrame\n",
    "                print(\"Read the CSV Dataset successfully\")\n",
    "\n",
    "                blkFriDF.reset_index(drop=True, inplace=True)\n",
    "                print(\"Black Friday Cleaned CSV retreived :\")\n",
    "                print(blkFriDF.head(3))\n",
    "\n",
    "\n",
    "                blkFriDF.rename(columns={'User_ID': 'user_id','Product_ID': 'product_id','Gender': 'gender','Age': 'age','Occupation': 'occupation',\n",
    "                        'City_Category': 'city_category','Stay_In_Current_City_Years': 'stay_in_current_city_years',\n",
    "                       'Marital_Status': 'marital_status','Product_Category_1': 'product_category_1',\n",
    "                       'Purchase': 'purchase'}, inplace=True)\n",
    "\n",
    "                print(\"Column Names After Renaming the column from the dataset\")\n",
    "                print(blkFriDF.columns)\n",
    "                \n",
    "                \n",
    "                \n",
    "                table_name = 'blackfri_tbl'\n",
    "                \n",
    "                # Prepare the insert query:\n",
    "                insert_query = sql.SQL(\"INSERT INTO {} ({}) VALUES ({})\").format(\n",
    "                    sql.Identifier(table_name),\n",
    "                    sql.SQL(', ').join(map(sql.Identifier, blkFriDF.columns)),\n",
    "                    sql.SQL(', ').join(sql.Placeholder() * len(blkFriDF.columns))\n",
    "                )\n",
    "                \n",
    "                # Converting the data from dataframe into list:\n",
    "                values = [tuple(x) for x in blkFriDF.to_numpy()]\n",
    "                # Insert the values into the table:\n",
    "                dbCursor.executemany(insert_query, values)\n",
    "                # This will do a autocommit    \n",
    "                dbConnstr.set_isolation_level(0) \n",
    "                \n",
    "                print(\"All the data has been saved to the blackfri_tbl tables successfully\")\n",
    "\n",
    "            except (Exception , psycopg2.Error) as dbError :\n",
    "                print (\"An error has occured while connecting to the PostgreSQL database: \", dbError)\n",
    "            finally:\n",
    "                # Closing the postgres connection:\n",
    "                if dbConnstr in locals():\n",
    "                    dbConnstr.close()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# Now we will save the cleaned dataset AppleStock price to Postgres\n",
    "class saveAppleStockDatatoPostgres(luigi.Task):\n",
    "    dataSetPath = luigi.Parameter()\n",
    "    PostGres_table_name = luigi.Parameter()\n",
    "    postgres_uri = luigi.Parameter()\n",
    "    user = luigi.Parameter()\n",
    "    password = luigi.Parameter()\n",
    "    host = luigi.Parameter()\n",
    "    port = luigi.Parameter()\n",
    "    database = luigi.Parameter()\n",
    "            \n",
    "    def run(self):\n",
    "\n",
    "    # The following code will establish a connection to Postgres and then create a database:\n",
    "        try:\n",
    "\n",
    "            print(\"\\n\")\n",
    "            print(\"Now Running Task 9 i.e Creating Tables for BlackFriday and Storing data in the postgres table..\")\n",
    "\n",
    "            table_create_string = \"\"\" \n",
    "            DROP TABLE IF EXISTS aapl_tbl;\n",
    "            CREATE TABLE IF NOT EXISTS aapl_tbl (\n",
    "            datecol DATE, \n",
    "            opencol numeric(20,8),\n",
    "            highcol numeric(20,8),\n",
    "            lowcol numeric(20,8),\n",
    "            closecol numeric(20,8),\n",
    "            adjclosecol numeric(20,8),\n",
    "            volume numeric(20,8)\n",
    "            ); \"\"\"\n",
    "            \n",
    "            dbConnstr = psycopg2.connect(\n",
    "                    user = self.user,\n",
    "                    password = self.password,\n",
    "                    host = self.host,\n",
    "                    port = self.port,\n",
    "                    database = self.database\n",
    "            \n",
    "            )\n",
    "            \n",
    "            # This will do a autocommit    \n",
    "            dbConnstr.set_isolation_level(0)\n",
    "            # The following code will create a cursor:\n",
    "            dbCursor = dbConnstr.cursor() \n",
    "            # The following code will create the table in the respective database:\n",
    "            dbCursor.execute(table_create_string)  \n",
    "            # Print the success message of table creation:\n",
    "            print(\"Table has been created successfully\")\n",
    "            \n",
    "            \n",
    "            # Now  Lets read the cleaned csv data and convert it to dataframe\n",
    "            # Path to the CSV file \n",
    "            apple_cleaned_data = self.dataSetPath\n",
    "            # Read the CSV file into a \n",
    "            appleDF = pd.read_csv(apple_cleaned_data) \n",
    "            print(\"\\n\")\n",
    "            print(\"apple DF Cleaned CSV retreived and read the CSV Dataset successfully\")\n",
    "            \n",
    "            print(appleDF.head(3)) \n",
    "            appleDF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            appleDF.rename(columns={'Date': 'datecol','Open': 'opencol', 'High': 'highcol','Low': 'lowcol','Close': 'closecol',\n",
    "                      'Adj Close': 'adjclosecol','Volume': 'volume'}, inplace=True)\n",
    "            \n",
    "    \n",
    "\n",
    "            \n",
    "            # My target table name :\n",
    "            table_name = 'aapl_tbl'\n",
    "            # here below we are creating the insert query:\n",
    "            insert_query = sql.SQL(\"INSERT INTO {} ({}) VALUES ({})\").format(\n",
    "                sql.Identifier(table_name),\n",
    "                sql.SQL(', ').join(map(sql.Identifier, appleDF.columns)),\n",
    "                sql.SQL(', ').join(sql.Placeholder() * len(appleDF.columns))\n",
    "            )\n",
    "            # Converting the data from dataframe into list:\n",
    "            values = [tuple(x) for x in appleDF.to_numpy()]\n",
    "            # Insert the values into the table:\n",
    "            dbCursor.executemany(insert_query, values)\n",
    "            # This will do a autocommit    \n",
    "            dbConnstr.set_isolation_level(0) \n",
    "            \n",
    "            print(\"All the data has been saved to the aapl_tbl tables successfully\")\n",
    "            \n",
    "        except (Exception , psycopg2.Error) as dbError :\n",
    "            print (\"An error has occured while connecting to the PostgreSQL database: \", dbError)\n",
    "        finally:\n",
    "            # Closing the postgres connection:\n",
    "            if dbConnstr in locals():\n",
    "                dbConnstr.close()            \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # Define the MongoDB Atlas URI\n",
    "#     uri = 'mongodb+srv://tvoneplus513:hzH5o78u2i5GxoYd@cluster0.7i7ifgj.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0'\n",
    "#     replace above with below connection string\n",
    "    uri = \"mongodb+srv://DAAP:ncidaap@atlascluster.tlybx6o.mongodb.net/?retryWrites=true&w=majority&appName=AtlasCluster\"\n",
    "    \n",
    "    postgres_uri = \"postgresql://dap:dap@localhost:5432/dap\"\n",
    "    \n",
    "    # Create an instance of the Luigi daap_task to save JSON data to 'black_fri' collection\n",
    "    daap_task1 = fromJsonToMongo(\n",
    "        myJsonFileloc='DATASETS/black_friday.json', \n",
    "        myconnString=uri,\n",
    "        dataBsName='black_fri_db',\n",
    "        collnName = 'black_fri',\n",
    "        taskno = '1'\n",
    "    )\n",
    "  \n",
    "    # Create an instance of the Luigi daap_task to save another JSON data to 'super_sls' collection\n",
    "    daap_task2 = fromJsonToMongo(\n",
    "        myJsonFileloc='DATASETS/superStore_analysis.json',\n",
    "        myconnString=uri,\n",
    "        dataBsName='superstore_sls_db',\n",
    "        collnName='super_sls',\n",
    "        taskno = '2'\n",
    "    )\n",
    "    \n",
    "    daap_task3 = readDatafrmAPI()\n",
    "    \n",
    "    daap_task4 = retrieveDataAndCreateDataFramesuperstore(\n",
    "        myconnString=uri,\n",
    "        dataBsName='superstore_sls_db',\n",
    "        collnName='super_sls',\n",
    "    )\n",
    "    \n",
    "    daap_task5 = retrieveDataAndCreateDataFrameBlackFri(\n",
    "        myconnString=uri,\n",
    "        dataBsName='black_fri_db',\n",
    "        collnName='black_fri',\n",
    "    )\n",
    "    \n",
    "    daap_task6 = createDatabaseinPostgres(\n",
    "            user = \"dap\",\n",
    "            password = \"dap\", \n",
    "            host = \"127.0.0.1\",\n",
    "            port = \"5432\",\n",
    "            database = \"postgres\")\n",
    "    \n",
    "\n",
    "    daap_task7 = saveSuperSalesDatatoPostgres(\n",
    "        dataSetPath='outputDataframes/superStore_cleaned_data.csv',\n",
    "        PostGres_table_name='superStoreSales',\n",
    "        postgres_uri=postgres_uri,\n",
    "            user = \"dap\",\n",
    "                    password = \"dap\",\n",
    "                    host = \"127.0.0.1\",\n",
    "                    port = \"5432\",\n",
    "                    database = \"daapproj\"\n",
    "        \n",
    "    )\n",
    "    \n",
    "    daap_task8 = saveBlackFridayDatatoPostgres(\n",
    "        dataSetPath= 'outputDataframes/black_friday_cleaned_data.csv',\n",
    "        PostGres_table_name='superStoreSales',\n",
    "        postgres_uri=postgres_uri,\n",
    "        user = 'dap',\n",
    "                    password = 'dap',\n",
    "                    host = \"127.0.0.1\",\n",
    "                    port = \"5432\",\n",
    "                    database = \"daapproj\"\n",
    "    )\n",
    "    \n",
    "    daap_task9 = saveAppleStockDatatoPostgres(\n",
    "        dataSetPath=\"outputDataframes/apple_cleaned_data.csv\",\n",
    "        PostGres_table_name='superStoreSales',\n",
    "        postgres_uri=postgres_uri,\n",
    "        user = \"dap\",\n",
    "                    password = \"dap\",\n",
    "                    host = \"127.0.0.1\",\n",
    "                    port = \"5432\",\n",
    "                    database = \"daapproj\"\n",
    "    \n",
    "    )\n",
    "    \n",
    "    # Run the Luigi daap_tasks using the local scheduler\n",
    "#     luigi.build([daap_task5 ,daap_task6,daap_task7,daap_task8,daap_task9], local_scheduler=True)\n",
    "#     luigi.build([daap_task1], local_scheduler=True)\n",
    "    \n",
    "    luigi.build([daap_task1, daap_task2, daap_task3, daap_task4,daap_task5 ,daap_task6, daap_task7,daap_task8,daap_task9], local_scheduler=True)\n",
    "\n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3911a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
